{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we left off the previous notebook with a question - \"could we optimize the data processing time?\" This is super important to investigate for the following reasons:\n",
    "- We are using a GPU for training our model. But could we use for processing the data chunks as well? Basically, while the model is training on batches of data there's a high chance that it can remain idle if the training data is available to it (and it can happen). \n",
    "- If we can optimize the data processing time, it will definitely result in faster training time as well because the model won't have to wait for the data to come to it. \n",
    "- This gives us an opportunity to mximize the resource utilization as well. Because there could be situations where the GPU/CPU is not being utilized fully. So while the model is training, in the background we could run some threads to fetch the next batch of data.\n",
    "\n",
    "Let's start the investigation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
