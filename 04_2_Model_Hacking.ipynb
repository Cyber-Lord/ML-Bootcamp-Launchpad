{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear any existing session\n",
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from wandb.keras import WandbCallback\n",
    "from utils import data_utils_v2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import wandb\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for better reproducibility\n",
    "tf.random.set_seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable XLA\n",
    "tf.config.optimizer.set_jit(True)\n",
    "\n",
    "# Enable AMP\n",
    "tf.keras.mixed_precision.experimental.set_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief introduction to XLA is available [here](https://docs.google.com/presentation/d/1F7hBey7m7bKSmLB4-Ipe9KvZl--TkaJGi69wRzzpAGM/edit#slide=id.p1). It helps to fuse certain operations (like addition, division, sqrt) used in a deep learning model thereby speeding up computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/sayakpaul/ML-Bootcamp-Launchpad\" target=\"_blank\">https://app.wandb.ai/sayakpaul/ML-Bootcamp-Launchpad</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/sayakpaul/ML-Bootcamp-Launchpad/runs/dao8p761\" target=\"_blank\">https://app.wandb.ai/sayakpaul/ML-Bootcamp-Launchpad/runs/dao8p761</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "W&B Run: https://app.wandb.ai/sayakpaul/ML-Bootcamp-Launchpad/runs/dao8p761"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize wandb\n",
    "wandb.init(\"ml-bootcamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this\n",
    "CLASSES = [b'daisy', b'dandelion', b'roses', b'sunflowers', b'tulips']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the constants\n",
    "BATCH_SIZE = 80\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load up the tfrecord filenames\n",
    "tfr_pattern_train = \"train_tfr/*.tfrec\"\n",
    "train_filenames = tf.io.gfile.glob(tfr_pattern_train)\n",
    "tfr_pattern_test = \"test_tfr/*.tfrec\"\n",
    "test_filenames = tf.io.gfile.glob(tfr_pattern_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the train and test dataset\n",
    "training_dataset, steps_per_epoch = data_utils_v2.batch_dataset(train_filenames, BATCH_SIZE, True)\n",
    "validation_dataset, validation_steps = data_utils_v2.batch_dataset(test_filenames, BATCH_SIZE, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a utility function which would return us an adjusted ResNet50 model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(img_size=(224,224), num_class=5, train_base=True):\n",
    "    input_layer = Input(shape=(img_size[0],img_size[1],3), dtype=tf.float16)\n",
    "    base = VGG16(input_tensor=input_layer,\n",
    "                    include_top=False,\n",
    "                    weights=\"imagenet\")\n",
    "    base.trainable = train_base\n",
    "    x = base.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    preds = Dense(num_class, activation=\"softmax\", dtype=tf.float32)(x)\n",
    "    return Model(inputs=input_layer, outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model, supply the loss scaled optimizer,\n",
    "# and compile it\n",
    "model = create_model()\n",
    "opt = Adam(learning_rate=1e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=opt,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "39/39 [==============================] - 49s 1s/step - loss: 1.1994 - accuracy: 0.5179 - val_loss: 0.5957 - val_accuracy: 0.7667\n",
      "Epoch 2/20\n",
      "39/39 [==============================] - 39s 997ms/step - loss: 0.6266 - accuracy: 0.7654 - val_loss: 0.3774 - val_accuracy: 0.8583\n",
      "Epoch 3/20\n",
      "39/39 [==============================] - 38s 966ms/step - loss: 0.4032 - accuracy: 0.8599 - val_loss: 0.2260 - val_accuracy: 0.9229\n",
      "Epoch 4/20\n",
      "39/39 [==============================] - 37s 957ms/step - loss: 0.3346 - accuracy: 0.8859 - val_loss: 0.3904 - val_accuracy: 0.8708\n",
      "Epoch 5/20\n",
      "39/39 [==============================] - 37s 959ms/step - loss: 0.3104 - accuracy: 0.8785 - val_loss: 0.2153 - val_accuracy: 0.9250\n",
      "Epoch 6/20\n",
      "39/39 [==============================] - 37s 960ms/step - loss: 0.2875 - accuracy: 0.8958 - val_loss: 0.1343 - val_accuracy: 0.9583\n",
      "Epoch 7/20\n",
      "39/39 [==============================] - 37s 952ms/step - loss: 0.2511 - accuracy: 0.9109 - val_loss: 0.1394 - val_accuracy: 0.9542\n",
      "Epoch 8/20\n",
      "39/39 [==============================] - 37s 957ms/step - loss: 0.2021 - accuracy: 0.9308 - val_loss: 0.1027 - val_accuracy: 0.9688\n",
      "Epoch 9/20\n",
      "39/39 [==============================] - 37s 950ms/step - loss: 0.1531 - accuracy: 0.9519 - val_loss: 0.1514 - val_accuracy: 0.9458\n",
      "Epoch 10/20\n",
      "39/39 [==============================] - 37s 957ms/step - loss: 0.1759 - accuracy: 0.9349 - val_loss: 0.0885 - val_accuracy: 0.9750\n",
      "Epoch 11/20\n",
      "39/39 [==============================] - 37s 955ms/step - loss: 0.1627 - accuracy: 0.9433 - val_loss: 0.0678 - val_accuracy: 0.9812\n",
      "Epoch 12/20\n",
      "39/39 [==============================] - 37s 946ms/step - loss: 0.1173 - accuracy: 0.9606 - val_loss: 0.0789 - val_accuracy: 0.9708\n",
      "Epoch 13/20\n",
      "39/39 [==============================] - 37s 951ms/step - loss: 0.0833 - accuracy: 0.9734 - val_loss: 0.0605 - val_accuracy: 0.9792\n",
      "Epoch 14/20\n",
      "39/39 [==============================] - 37s 952ms/step - loss: 0.0685 - accuracy: 0.9769 - val_loss: 0.0309 - val_accuracy: 0.9937\n",
      "Epoch 15/20\n",
      "39/39 [==============================] - 37s 942ms/step - loss: 0.0780 - accuracy: 0.9705 - val_loss: 0.0924 - val_accuracy: 0.9708\n",
      "Epoch 16/20\n",
      "39/39 [==============================] - 37s 945ms/step - loss: 0.1417 - accuracy: 0.9542 - val_loss: 0.0368 - val_accuracy: 0.9958\n",
      "Epoch 17/20\n",
      "39/39 [==============================] - 37s 943ms/step - loss: 0.0838 - accuracy: 0.9763 - val_loss: 0.4407 - val_accuracy: 0.8542\n",
      "Epoch 18/20\n",
      "39/39 [==============================] - 37s 948ms/step - loss: 0.2089 - accuracy: 0.9224 - val_loss: 0.1123 - val_accuracy: 0.9646\n",
      "Epoch 19/20\n",
      "39/39 [==============================] - 37s 941ms/step - loss: 0.1065 - accuracy: 0.9631 - val_loss: 0.0385 - val_accuracy: 0.9896\n",
      "Epoch 20/20\n",
      "39/39 [==============================] - 37s 938ms/step - loss: 0.0774 - accuracy: 0.9740 - val_loss: 0.0413 - val_accuracy: 0.9854\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "start = time.time()\n",
    "model.fit_generator(training_dataset, \n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=validation_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[WandbCallback(data_type=\"image\", labels=CLASSES)])\n",
    "wandb.log({\"training_time\": time.time() - start})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model trains much faster and still it is performing :)\n",
    "\n",
    "A comparative study on mixed precision training is available [here](https://github.com/sayakpaul/Mixed-Precision-Training-in-tf.keras-2.0).\n",
    "\n",
    "We are going to export our Keras model to `SavedModel` format. But why this format?\n",
    "\n",
    "> SavedModel is a standalone serialization format for TensorFlow objects, supported by TensorFlow serving as well as TensorFlow implementations other than Python. - [Save and serialize models with Keras](https://www.tensorflow.org/guide/keras/save_and_serialize)\n",
    "\n",
    "`tf.keras` offers a lot of options to serialize models and this a great read in that line: https://www.tensorflow.org/guide/keras/save_and_serialize. \n",
    "\n",
    "First, let's convert the Keras model to TensorFlow estimator and then convert it to SavedModel format. `tf.keras` allows for direct conversion to `SavedModel` using `model.save(dir_name, save_format=\"tf\")` but the input shapes of our model is not supported for that direct conversion. So, we need the extra step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The warnings can be ignored*. We will now write a serving preprocessing function which will be appended to our model's graph helping us to serve predictions efficiently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input_1'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the name of the first layer of the model\n",
    "model_input_name = model.input_names[0]\n",
    "model_input_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function courtesy: http://bit.ly/2YxFbzN\n",
    "def serving_input_receiver_fn():\n",
    "    # initialize a placeholder to recieve the image\n",
    "    input_ph = tf.compat.v1.placeholder(tf.string, shape=[None], name='image_binary')\n",
    "    # map the image with decode_image to cast it to uint8 dtype\n",
    "    images = tf.map_fn(tf.image.decode_image, input_ph, dtype=tf.uint8)\n",
    "    # cast the pixels to float32 and scale the values, and cast back \n",
    "    # to float16\n",
    "    images = tf.cast(images, tf.float32) / 255.\n",
    "    images = tf.cast(images, tf.float16)\n",
    "    \n",
    "    # set the dimensions (None because we don't know how many images would come)\n",
    "    images.set_shape([None, 224, 224, 3])\n",
    "\n",
    "    # 1st: what will be going into the model\n",
    "    # 2nd: what comes to the model initially\n",
    "    return tf.estimator.export.ServingInputReceiver({model_input_name: images}, {'bytes': input_ph})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from estimator_model/keras/keras_model.ckpt\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: saved_model/temp-b'1575801062'/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'saved_model/1575801062'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert TF estimator to SavedModel\n",
    "export_path = estimator_model.export_saved_model('saved_model', serving_input_receiver_fn=serving_input_receiver_fn)\n",
    "export_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
