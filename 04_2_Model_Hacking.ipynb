{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear any existing session\n",
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from wandb.keras import WandbCallback\n",
    "from utils import data_utils\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import wandb\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for better reproducibility\n",
    "tf.random.set_seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable XLA\n",
    "tf.config.optimizer.set_jit(True)\n",
    "\n",
    "# Enable AMP\n",
    "tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief introduction to XLA is available [here](https://docs.google.com/presentation/d/1F7hBey7m7bKSmLB4-Ipe9KvZl--TkaJGi69wRzzpAGM/edit#slide=id.p1). It helps to fuse certain operations (like addition, division, sqrt) used in a deep learning model thereby speeding up computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/sayakpaul/ML-Bootcamp-Launchpad\" target=\"_blank\">https://app.wandb.ai/sayakpaul/ML-Bootcamp-Launchpad</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/sayakpaul/ML-Bootcamp-Launchpad/runs/436jaav7\" target=\"_blank\">https://app.wandb.ai/sayakpaul/ML-Bootcamp-Launchpad/runs/436jaav7</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "W&B Run: https://app.wandb.ai/sayakpaul/ML-Bootcamp-Launchpad/runs/436jaav7"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize wandb\n",
    "wandb.init(\"ml-bootcamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this\n",
    "CLASSES = [b'daisy', b'dandelion', b'roses', b'sunflowers', b'tulips']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the constants\n",
    "BATCH_SIZE = 80\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load up the tfrecord filenames\n",
    "tfr_pattern_train = \"train_tfr/*.tfrec\"\n",
    "train_filenames = tf.io.gfile.glob(tfr_pattern_train)\n",
    "tfr_pattern_test = \"test_tfr/*.tfrec\"\n",
    "test_filenames = tf.io.gfile.glob(tfr_pattern_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the train and test dataset\n",
    "training_dataset, steps_per_epoch = data_utils.batch_dataset(train_filenames, BATCH_SIZE, True)\n",
    "validation_dataset, validation_steps = data_utils.batch_dataset(test_filenames, BATCH_SIZE, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a utility function which would return us an adjusted ResNet50 model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(img_size=(224,224), num_class=5, train_base=True):\n",
    "    input_layer = Input(shape=(img_size[0],img_size[1],3))\n",
    "    base = ResNet50(input_tensor=input_layer,\n",
    "                    include_top=False,\n",
    "                    weights=\"imagenet\")\n",
    "    base.trainable = train_base\n",
    "    x = base.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    preds = Dense(num_class, activation=\"softmax\")(x)\n",
    "    return Model(inputs=input_layer, outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model, supply the loss scaled optimizer,\n",
    "# and compile it\n",
    "model = create_model()\n",
    "opt = Adam(learning_rate=1e-4)\n",
    "opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt,  \n",
    "                                                       \"dynamic\")\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=opt,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "39/39 [==============================] - 65s 2s/step - loss: 0.4421 - accuracy: 0.8452 - val_loss: 1.9439 - val_accuracy: 0.2417\n",
      "Epoch 2/20\n",
      "39/39 [==============================] - 56s 1s/step - loss: 0.1455 - accuracy: 0.9529 - val_loss: 2.4620 - val_accuracy: 0.2417\n",
      "Epoch 3/20\n",
      "39/39 [==============================] - 54s 1s/step - loss: 0.0469 - accuracy: 0.9881 - val_loss: 2.2336 - val_accuracy: 0.2417\n",
      "Epoch 4/20\n",
      "39/39 [==============================] - 54s 1s/step - loss: 0.0398 - accuracy: 0.9869 - val_loss: 2.4295 - val_accuracy: 0.2417\n",
      "Epoch 5/20\n",
      "39/39 [==============================] - 54s 1s/step - loss: 0.0294 - accuracy: 0.9926 - val_loss: 2.0264 - val_accuracy: 0.2438\n",
      "Epoch 6/20\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "start = time.time()\n",
    "model.fit_generator(training_dataset, \n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=validation_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[WandbCallback(data_type=\"image\", labels=CLASSES)])\n",
    "wandb.log({\"training_time\": time.time() - start})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A comparative study on mixed precision training is available [here](https://github.com/sayakpaul/Mixed-Precision-Training-in-tf.keras-2.0)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
